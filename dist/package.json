{
  "name": "gs-tokenizer",
  "version": "0.1.14",
  "type": "module",
  "main": "./lib/index.cjs",
  "module": "./lib/index.js",
  "exports": {
    ".": {
      "types": "./lib/index.d.ts",
      "import": "./lib/index.js",
      "require": "./lib/index.cjs"
    },
    "./core": {
      "types": "./lib/core.d.ts",
      "import": "./lib/core.js",
      "require": "./lib/core.cjs"
    },
    "./old": {
      "types": "./lib/old.d.ts",
      "import": "./lib/old.js",
      "require": "./lib/old.cjs"
    },
    "./lexicon": {
      "types": "./lib/lexicon.d.ts",
      "import": "./lib/lexicon.js",
      "require": "./lib/lexicon.cjs"
    }
  },
  "keywords": [
    "tokenizer",
    "multilingual",
    "Chinese",
    "Japanese",
    "Korean",
    "English",
    "NLP",
    "Natural Language Processing",
    "text processing"
  ],
  "homepage": "https://github.com/grain-sand/gs-tokenizer",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/grain-sand/gs-tokenizer.git"
  },
  "bugs": {
    "url": "https://github.com/grain-sand/gs-tokenizer/issues"
  },
  "author": "",
  "license": "MIT",
  "dependencies": {}
}
