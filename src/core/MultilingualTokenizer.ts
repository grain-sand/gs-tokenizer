import {
	DefaultTokenizerOption,
	IMultilingualTokenizer,
	INameLexiconGroup,
	IRange,
	ISpanToken,
	IToken, ITokenizerOption,
	ITokenizerStage,
	SupportedLanguage,
	TokenType,
} from "../type";
import {FirstCharWordIndex} from "./FirstCharWordIndex";
import {DictionaryStage} from "./DictionaryStage";
import {NameCnStage} from "./Name/NameCnStage";
import {NameJkStage} from "./Name/NameJkStage";
import {NameOtherStage} from "./Name/NameOtherStage";
import {NumberStage} from "./RegexArray/NumberStage";
import {SpaceStage} from "./RegexArray/SpaceStage";
import {PunctuationStage} from "./RegexArray/PunctuationStage";
import {SocialStage} from "./SocialStage";
import {EmailStage} from "./EmailStage";
import {DateStage} from "./RegexArray/DateStage";
import {UrlStage} from "./RegexArray/UrlStage";
import {IpStage} from "./RegexArray/IpStage";
import {tokenText} from "./util/tokenText";
import {detectLang} from "./util/detectLang";


export class MultilingualTokenizer implements IMultilingualTokenizer {

	readonly wordIndex = new FirstCharWordIndex();
	#stages: ITokenizerStage[] = [];
	#lexiconNames = new Set<string>();
	#nameLexiconNames: string[] = [];

	#urlStage?: UrlStage;

	#nativeSegmenter =
		typeof Intl !== 'undefined' && 'Segmenter' in Intl
			? new Intl.Segmenter('und', {granularity: 'word'})
			: null;

	#option!: Required<ITokenizerOption>;

	constructor(option?: ITokenizerOption) {
		this.initialize({...DefaultTokenizerOption, ...option});
	}

	get loadedLexiconNames(): string[] {
		return [...this.#lexiconNames];
	}

	get loadedNameLexiconNames(): string[] {
		return this.#nameLexiconNames;
	}

	initialize(option?: ITokenizerOption): void {
		// åˆå¹¶é€‰é¡¹ï¼Œä½¿ç”¨é»˜è®¤å€¼ä½œä¸ºåŸºç¡€
		const mergedOption = {...DefaultTokenizerOption, ...this.#option, ...option};
		// æ–­è¨€ä¸ºRequiredç±»å‹ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åˆå¹¶äº†é»˜è®¤å€¼
		const validatedOption = mergedOption as Required<ITokenizerOption>;

		// éªŒè¯é€‰é¡¹å€¼èŒƒå›´
		if (validatedOption.minTokenLength < 0) {
			throw new Error('minTokenLength must be greater than or equal to 0');
		}
		if (validatedOption.cjkTokenLengthLimit <= 0) {
			throw new Error('cjkTokenLengthLimit must be greater than 0');
		}
		if (validatedOption.enTokenLengthLimit <= 0) {
			throw new Error('enTokenLengthLimit must be greater than 0');
		}
		if (validatedOption.urlPathLengthLimit <= 0) {
			throw new Error('urlPathLengthLimit must be greater than 0');
		}
		if (validatedOption.urlQueryLengthLimit <= 0) {
			throw new Error('urlQueryLengthLimit must be greater than 0');
		}

		// éªŒè¯cjkTokenLengthLimit < enTokenLengthLimit
		if (validatedOption.cjkTokenLengthLimit >= validatedOption.enTokenLengthLimit) {
			throw new Error('cjkTokenLengthLimit must be less than enTokenLengthLimit');
		}

		// è®¾ç½®éªŒè¯åçš„é€‰é¡¹
		this.#option = validatedOption;

		if (this.#urlStage) {
			this.#urlStage.setOption(this.#option);
			return;
		}
		this.addStage(new DictionaryStage());
		this.addStage(new SocialStage());
		this.addStage(new EmailStage());
		this.addStage(this.#urlStage = new UrlStage(validatedOption));
		this.addStage(new IpStage());
		this.addStage(new DateStage());
		this.addStage(new NumberStage());
		this.addStage(new PunctuationStage());
		this.addStage(new SpaceStage());
	}

	addDictionary(
		words: string[],
		name: string,
		priority = 0,
		language?: SupportedLanguage
	) {
		this.#lexiconNames.add(name);
		// ç›´æ¥ä½¿ç”¨addBatchç¡®ä¿æ‰¹é‡æ·»åŠ çš„åŸå­æ€§
		this.wordIndex.addBatch(
			words.map(w => ({word: w, meta: {name, priority, lang: language}}))
		);
	}

	setNameDictionary(group: INameLexiconGroup, language: SupportedLanguage) {
		this.#nameLexiconNames.push(language);
		if (/^zh/i.test(language)) {
			this.addStage(new NameCnStage(group, language));
		} else if (/^(ko|jp)/i.test(language)) {
			this.addStage(new NameJkStage(group, language));
		} else {
			this.addStage(new NameOtherStage(group, language));
		}
	}

	addStage(stage: ITokenizerStage) {
		this.#stages.push(stage);
		this.#stages.sort(
			(a, b) => a.order - b.order || b.priority - a.priority
		);
		stage.initialize?.(this);
	}

	tokenize(text: string): ISpanToken[] {
		// åˆ†è¯å‰å¤„ç†ï¼šå¦‚æœéœ€è¦è½¬å°å†™ï¼Œç›´æ¥å¯¹æ•´ä¸ªæ–‡æœ¬è½¬å°å†™
		if (this.#option.lowercaseEnglish) {
			text = text.toLowerCase();
		}

		const tokens: ISpanToken[] = [];
		const len = text.length;

		let pos = 0;

		while (pos < len) {
			let advanced = false;

			for (const stage of this.#stages) {
				const r = stage.best(text, pos);
				if (!r.tokens.length) continue;

				for (const t of r.tokens) {
					tokens.push({
						...t,
						start: pos,
						end: r.unprocessedStart
					});
				}

				pos = r.unprocessedStart;
				advanced = true;
				break;
			}

			if (!advanced) {
				pos++;
			}
		}

		// ğŸ”§ ç”¨ span è¡¥é½æ‰€æœ‰è¢«è·³è¿‡çš„åŒºé—´
		let result = this.#filTokenizeGapsWithNative(text, tokens);

		// åº”ç”¨ ITokenizerOption é€»è¾‘
		result = this.#applyTokenizerOptions(result) as ISpanToken[];

		return result;
	}

	tokenizeAll(text: string): IToken[] {
		// åˆ†è¯å‰å¤„ç†ï¼šå¦‚æœéœ€è¦è½¬å°å†™ï¼Œç›´æ¥å¯¹æ•´ä¸ªæ–‡æœ¬è½¬å°å†™
		if (this.#option.lowercaseEnglish) {
			text = text.toLowerCase();
		}

		let pos = 0;
		const rangeTokens: [IRange, IToken[]][] = [];
		const lastMap = new Map<ITokenizerStage, number>();
		let processedPos = 0;

		while (pos < text.length) {
			const substr = text.slice(pos);
			const tokens: IToken[] = [];
			let skip = 1;
			for (const stage of this.#stages) {

				if (
					processedPos > pos && stage.unprocessedOnly
					|| stage.skipOwnLastMax && pos < lastMap.get(stage)!
				) {
					continue;
				}

				const result = stage.all(substr);
				if (!result.end) continue;

				tokens.push(...result.tokens);
				let last = result.end;
				processedPos = Math.max(processedPos, pos + last);
				if (stage.skipOwnLastMax) {
					lastMap.set(stage, pos + last);
				}

				if (stage.breakIfProcessed) {
					skip = last;
					break;
				}

			}
			if (tokens.length) {
				const range = {start: pos, end: processedPos};
				rangeTokens.push([range, tokens]);
			}
			pos += skip;
		}
		// åº”ç”¨ ITokenizerOption é€»è¾‘
		let result = this.#filTokenizeAllGapsWithNative(text, rangeTokens);
		result = this.#applyTokenizerOptions(result);
		return result;
	}

	tokenizeText(text: string, exclude?: TokenType[]): string[] {
		return tokenText(this.tokenize(text), exclude);
	}

	tokenizeTextAll(text: string, exclude?: TokenType[]): string[] {
		return tokenText(this.tokenizeAll(text), exclude);
	}

	#applyTokenizerOptions(tokens: (IToken | ISpanToken)[]): (IToken | ISpanToken)[] {
		const {minTokenLength, cjkTokenLengthLimit, enTokenLengthLimit} = this.#option;

		// å®šä¹‰CJKå­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
		const cjkRegex = /[\p{Script=Han}\p{Script=Hiragana}\p{Script=Katakana}\p{Script=Hangul}]/u;

		const result: (IToken | ISpanToken)[] = [];
		for (let i = 0; i < tokens.length; i++) {
			const token = tokens[i];
			const tokenLength = token.txt.length;

			// å¿«é€Ÿè·³è¿‡ä¸éœ€è¦å¤„ç†çš„token
			if (tokenLength <= cjkTokenLengthLimit) {
				result.push(token);
				continue;
			}

			if (tokenLength < minTokenLength) {
				continue;
			}

			const isURLToken = token.type === 'host' || (token as IToken).src === 'url-path' || (token as IToken).src === 'url-query-string';


			// ä¼˜å…ˆå¤„ç†URLç›¸å…³token
			if (isURLToken) {
				result.push(token);
				continue;
			}
			const isCJK = cjkRegex.test(token.txt) || (token as any).meta?.lang?.startsWith('zh') || (token as any).meta?.lang?.startsWith('ja') || (token as any).meta?.lang?.startsWith('ko');

			// å¯¹è¶…è¿‡é•¿åº¦é™åˆ¶çš„tokenè¿›è¡Œæˆªæ–­å¤„ç†
			if (isCJK && tokenLength > cjkTokenLengthLimit) {
				// CJKå­—ç¬¦æŒ‰cjkTokenLengthLimitæˆªæ–­
				token.txt = token.txt.slice(0, cjkTokenLengthLimit);
			} else if (!isCJK && tokenLength > enTokenLengthLimit) {
				// éCJKå­—ç¬¦æŒ‰enTokenLengthLimitæˆªæ–­
				token.txt = token.txt.slice(0, enTokenLengthLimit);
			}

			result.push(token);
		}

		return result;
	}

	#filTokenizeAllGapsWithNative(text: string, rangeTokens: [IRange, IToken[]][]): IToken[] {
		const out: IToken[] = [];
		let cursor = 0;

		if (rangeTokens.length) for (const [t, tokens] of rangeTokens) {
			if (cursor < t.start) {
				out.push(...this.#nativeSegment(text, cursor, t.start)
				);
			}
			out.push(...tokens);
			cursor = t.end;
		}

		if (cursor < text.length) {
			out.push(
				...this.#nativeSegment(text, cursor, text.length)
			);
		}

		return out;
	}

	#filTokenizeGapsWithNative(
		text: string,
		tokens: ISpanToken[],
	): ISpanToken[] {
		if (!this.#nativeSegmenter) return tokens;

		const out: ISpanToken[] = [];
		let cursor = 0;

		for (const t of tokens) {
			if (cursor < t.start) {
				out.push(
					...this.#nativeSegment(text, cursor, t.start)
				);
			}
			out.push(t);
			cursor = t.end;
		}

		if (cursor < text.length) {
			out.push(
				...this.#nativeSegment(text, cursor, text.length)
			);
		}

		return out;
	}

	#nativeSegment(
		text: string,
		start: number,
		end: number
	): ISpanToken[] {
		const slice = start === 0 && end === text.length ? text : text.slice(start, end);
		const res: ISpanToken[] = [];

		for (const seg of this.#nativeSegmenter!.segment(slice)) {
			const s = start + seg.index;
			const e = s + seg.segment.length;

			res.push({
				txt: seg.segment,
				type: seg.isWordLike ? 'word' : 'other',
				src: 'native',
				lang: detectLang(seg.segment),
				start: s,
				end: e
			});
		}

		return res;
	}
}
