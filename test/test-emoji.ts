import {beforeEach, describe, expect, it} from "vitest";
import { MultilingualTokenizer } from "../src";

// // const console = (top as any).console;

describe('Multilingual Tokenizer - Emoji Tests', () => {
  let tokenizer: MultilingualTokenizer;

  beforeEach(() => {
    tokenizer = new MultilingualTokenizer();
  });

  it('should tokenize single emoji correctly', () => {
    const text = 'ðŸ‘‹';
    console.log('Testing single emoji:', text);
    
    const tokens = tokenizer.tokenize(text);
    const emojiTokens = tokens.filter(token => token.type === 'emoji');
    
    console.log('Single emoji tokens:', tokens);
    console.log('Emoji tokens found:', emojiTokens);
    
    // éªŒè¯å­˜åœ¨emojiç±»åž‹çš„token
    const hasEmoji = tokens.some(token => /\p{Emoji}/u.test(token.txt));
    expect(hasEmoji).toBe(true);
  });

  it('should handle continuous emojis', () => {
    const text = 'â¤ï¸ðŸ”¥âœ¨';
    console.log('Testing continuous emojis:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Continuous emojis tokens:', tokens);
    
    // éªŒè¯åŒ…å«emojiçš„tokenå­˜åœ¨
    const hasEmojiTokens = tokens.some(token => /\p{Emoji}/u.test(token.txt));
    expect(hasEmojiTokens).toBe(true);
  });

  it('should handle emojis with modifiers', () => {
    const text = 'ðŸ‘ðŸ»ðŸ‘ŽðŸ¿ðŸ‘ðŸ½';
    console.log('Testing emojis with modifiers:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Emojis with modifiers tokens:', tokens);
    
    // éªŒè¯åŒ…å«å¸¦ä¿®é¥°ç¬¦emojiçš„tokenå­˜åœ¨
    const hasEmojiTokens = tokens.some(token => /\p{Emoji}/u.test(token.txt));
    expect(hasEmojiTokens).toBe(true);
  });

  it('should not misclassify numbers as emojis', () => {
    const text = '12345';
    console.log('Testing numbers not as emojis:', text);
    
    const tokens = tokenizer.tokenize(text);
    const nonEmojiTokens = tokens.filter(token => token.type !== 'emoji');
    
    console.log('Numbers tokens:', tokens);
    
    // éªŒè¯æ‰€æœ‰tokenéƒ½ä¸æ˜¯emojiç±»åž‹
    expect(nonEmojiTokens.length).toBe(tokens.length);
  });

  it('should not misclassify mixed characters as emojis', () => {
    const text = 'user123';
    console.log('Testing mixed characters not as emojis:', text);
    
    const tokens = tokenizer.tokenize(text);
    const wordTokens = tokens.filter(token => token.type === 'word');
    
    console.log('Mixed characters tokens:', tokens);
    
    // éªŒè¯æ··åˆå­—ç¬¦è¢«è¯†åˆ«ä¸ºå•è¯è€Œä¸æ˜¯emoji
    expect(wordTokens.length).toBeGreaterThan(0);
  });

  it('should handle emojis in mixed text', () => {
    const text = 'Hello ðŸ‘‹ World!';
    console.log('Testing emojis in mixed text:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Mixed text with emojis tokens:', tokens);
    
    // éªŒè¯æ–‡æœ¬ä¸­åŒ…å«emoji
    const hasEmoji = tokens.some(token => /\p{Emoji}/u.test(token.txt));
    expect(hasEmoji).toBe(true);
    
    // éªŒè¯åŒæ—¶åŒ…å«å•è¯å’Œemoji
    const hasWords = tokens.some(token => token.type === 'word');
    expect(hasWords).toBe(true);
  });

  it('should handle emojis with leading spaces', () => {
    const text = ' ðŸ‘‹';
    console.log('Testing emoji with leading spaces:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Emoji with leading spaces tokens:', tokens);
    
    // éªŒè¯åŒ…å«emojiç±»åž‹çš„token
    const emojiTokens = tokens.filter(token => token.type === 'emoji');
    expect(emojiTokens.length).toBe(1);
    
    // éªŒè¯åŒ…å«spaceç±»åž‹çš„token
    const spaceTokens = tokens.filter(token => token.type === 'space');
    expect(spaceTokens.length).toBe(1);
  });

  it('should handle emojis with trailing spaces', () => {
    const text = 'ðŸ‘‹ ';
    console.log('Testing emoji with trailing spaces:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Emoji with trailing spaces tokens:', tokens);
    
    // éªŒè¯åŒ…å«emojiç±»åž‹çš„token
    const emojiTokens = tokens.filter(token => token.type === 'emoji');
    expect(emojiTokens.length).toBe(1);
    
    // éªŒè¯åŒ…å«spaceç±»åž‹çš„token
    const spaceTokens = tokens.filter(token => token.type === 'space');
    expect(spaceTokens.length).toBe(1);
  });

  it('should handle emojis with leading and trailing spaces', () => {
    const text = ' ðŸ‘‹ ';
    console.log('Testing emoji with leading and trailing spaces:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Emoji with leading and trailing spaces tokens:', tokens);
    
    // éªŒè¯åŒ…å«emojiç±»åž‹çš„token
    const emojiTokens = tokens.filter(token => token.type === 'emoji');
    expect(emojiTokens.length).toBe(1);
    
    // éªŒè¯åŒ…å«spaceç±»åž‹çš„token
    const spaceTokens = tokens.filter(token => token.type === 'space');
    expect(spaceTokens.length).toBe(2);
  });

  it('should handle continuous emojis with spaces', () => {
    const text = ' ðŸ‘‹ðŸ‘‹ ';
    console.log('Testing continuous emojis with spaces:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Continuous emojis with spaces tokens:', tokens);
    
    // éªŒè¯åŒ…å«emojiç±»åž‹çš„token
    const emojiTokens = tokens.filter(token => token.type === 'emoji');
    expect(emojiTokens.length).toBe(1);
    expect(emojiTokens[0].txt).toBe('ðŸ‘‹ðŸ‘‹');
    
    // éªŒè¯åŒ…å«spaceç±»åž‹çš„token
    const spaceTokens = tokens.filter(token => token.type === 'space');
    expect(spaceTokens.length).toBe(2);
  });

  it('should handle text with spaced emojis in between', () => {
    const text = 'Hello  ðŸ‘‹  World';
    console.log('Testing text with spaced emojis:', text);
    
    const tokens = tokenizer.tokenize(text);
    
    console.log('Text with spaced emojis tokens:', tokens);
    
    // éªŒè¯åŒ…å«emojiç±»åž‹çš„token
    const emojiTokens = tokens.filter(token => token.type === 'emoji');
    expect(emojiTokens.length).toBe(1);
    
    // éªŒè¯åŒ…å«spaceç±»åž‹çš„token
    const spaceTokens = tokens.filter(token => token.type === 'space');
    expect(spaceTokens.length).toBe(2);
    
    // éªŒè¯åŒ…å«wordç±»åž‹çš„token
    const wordTokens = tokens.filter(token => token.type === 'word');
    expect(wordTokens.length).toBe(2);
  });
});
